{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refrerences:\n",
    "- https://github.com/commaai/openpilot/tree/3766830f6588df6a1fee3e0295c689f9de364476\n",
    "- https://github.com/MTammvee/openpilot-supercombo-model/tree/main\n",
    "- https://github.com/noobmasterbala/Adversarial-Attack-and-Defence-On-Openpilot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Activate virtual environment (.\\venv\\Scripts\\activate)\n",
    "2. `pip3 install -r requirements.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pip packages\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "import onnx\n",
    "import onnxruntime\n",
    "from onnx2torch import convert\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Load Supercombo ONNX model\n",
    "model_name = \"supercombo_0.8.3.onnx\"\n",
    "onnx_model = onnx.load(model_name)\n",
    "\n",
    "# Conver to PyTorch model\n",
    "torch_model = convert(onnx_model)\n",
    "if torch.cuda.is_available():\n",
    "    torch_model.cuda()\n",
    "#torch_model.half() # Make the model use float16 dtype\n",
    "torch_model.eval()\n",
    "#print(torch_model)\n",
    "\n",
    "# Session ONNX\n",
    "session = onnxruntime.InferenceSession(model_name, providers=['CPUExecutionProvider'])\n",
    "\n",
    "def rgb_to_yuv(rgb_tensor):\n",
    "    # Ensure tensor is in (N, C, H, W) format\n",
    "    assert rgb_tensor.dim() == 4 and rgb_tensor.size(1) == 3, \"Input tensor must be in (N, C, H, W) format with 3 channels\"\n",
    "\n",
    "    # Convert RGB to YUV\n",
    "    R = rgb_tensor[:, 0, :, :]\n",
    "    G = rgb_tensor[:, 1, :, :]\n",
    "    B = rgb_tensor[:, 2, :, :]\n",
    "\n",
    "    Y = 0.299 * R + 0.587 * G + 0.114 * B\n",
    "    U = -0.14713 * R - 0.28886 * G + 0.436 * B\n",
    "    V = 0.614 * R - 0.51498 * G - 0.10001 * B\n",
    "\n",
    "    yuv_tensor = torch.stack([Y, U, V], dim=1)\n",
    "    return yuv_tensor\n",
    "\n",
    "def parse_image(frame):\n",
    "    # Ensure frame is a tensor of shape (1, 3, H, W)\n",
    "    assert frame.dim() == 4 and frame.size(1) == 3, \"Input tensor must be of shape (1, 3, H, W)\"\n",
    "    \n",
    "    H = frame.size(2)\n",
    "    W = frame.size(3)\n",
    "    \n",
    "    # Initialize the parsed tensor with shape (6, H//2, W//2)\n",
    "    parsed = torch.zeros((6, H//2, W//2), dtype=torch.uint8)\n",
    "    \n",
    "    # Extract the channels from the input tensor\n",
    "    Y = frame[0, 0, :, :]\n",
    "    U = frame[0, 1, :, :]\n",
    "    V = frame[0, 2, :, :]\n",
    "\n",
    "    # Populate the parsed tensor\n",
    "    parsed[0] = Y[0:H:2, 0::2]\n",
    "    parsed[1] = Y[1:H:2, 0::2]\n",
    "    parsed[2] = Y[0:H:2, 1::2]\n",
    "    parsed[3] = Y[1:H:2, 1::2]\n",
    "    parsed[4] = U[0:H//2, 0::2]  # assuming U is already appropriately sized\n",
    "    parsed[5] = V[0:H//2, 0::2]  # assuming V is already appropriately sized\n",
    "    \n",
    "    return parsed.unsqueeze(0)\n",
    "\n",
    "def seperate_points_and_std_values(df):\n",
    "\tpoints = df.iloc[lambda x: x.index % 2 == 0]\n",
    "\tstd = df.iloc[lambda x: x.index % 2 != 0]\n",
    "\tpoints = pd.concat([points], ignore_index = True)\n",
    "\tstd = pd.concat([std], ignore_index = True)\n",
    "\treturn points, std\n",
    "\n",
    "def display_image(image):\n",
    "\tplt.imshow(image)\n",
    "\tplt.show()\n",
    "\tplt.clf()\n",
    "\n",
    "def parse_input(session, imgs_array):\n",
    "\t# Retrieve input names\n",
    "\tinput_imgs = session.get_inputs()[0].name # ROI (Region Of Interest) area\n",
    "\tbig_input_imgs = session.get_inputs()[1].name # Wide frame\n",
    "\tdesire = session.get_inputs()[2].name\n",
    "\ttraffic_convention = session.get_inputs()[3].name\n",
    "\tnav_features = session.get_inputs()[4].name\n",
    "\tfeatures_buffer = session.get_inputs()[5].name\n",
    "\n",
    "\t# ORT_TYPES_TO_NP_TYPES = {'tensor(float16)': np.float16, 'tensor(float)': np.float32, 'tensor(uint8)': np.uint8}\n",
    "\t# ready to run onnx model ['input_imgs', 'big_input_imgs', 'desire', 'traffic_convention', 'nav_features', 'features_buffer'] [[1, 12, 128, 256], [1, 12, 128, 256], [1, 100, 8], [1, 2], [1, 256], [1, 99, 128]]\n",
    "\t# Prepare input data: https://github.com/commaai/openpilot/tree/fa310d9e2542cf497d92f007baec8fd751ffa99c/selfdrive/modeld/models\n",
    "\tinput_imgs_data = np.array(imgs_array[0]).astype('float16')\n",
    "\tinput_imgs_data.resize((1,12,128,256)) # [1, 12, 128, 256]\n",
    "\tbig_input_imgs_data = np.array(imgs_array[1]).astype('float16')\n",
    "\tbig_input_imgs_data.resize((1,12,128,256)) # [1, 12, 128, 256]\n",
    "\tdesire_data = np.array([0]).astype('float16') \n",
    "\tdesire_data.resize((1,100,8)) # [1, 100, 8]\n",
    "\ttraffic_convention_data = np.array([1, 0]).astype('float16')\n",
    "\ttraffic_convention_data.resize((1,2)) # [1, 2]\n",
    "\tnav_features_data = np.array([0]).astype('float16')\n",
    "\tnav_features_data.resize((1,256)) # [1, 256]\n",
    "\tfeatures_buffer_data = np.array([0]).astype('float16') \n",
    "\tfeatures_buffer_data.resize((1,99,128)) # [1, 99, 128]\n",
    "\t\n",
    "\treturn {input_imgs: input_imgs_data,\n",
    "\t\t\tbig_input_imgs: big_input_imgs_data,\n",
    "\t\t\tdesire: desire_data,\n",
    "\t\t\ttraffic_convention: traffic_convention_data,\n",
    "\t\t\tnav_features: nav_features_data,\n",
    "\t\t\tfeatures_buffer: features_buffer_data}\n",
    "\n",
    "def parse_output(output):\n",
    "\t# output tensor shape (6120, )\n",
    "\t# Model Output: https://github.com/commaai/openpilot/blob/fa310d9e2542cf497d92f007baec8fd751ffa99c/selfdrive/modeld/models/driving.h#L239\n",
    "\tplan_idx_start = 0\n",
    "\tplan_idx_end = 4955\n",
    "\tlane_idx_start = plan_idx_end\n",
    "\tlane_idx_end = lane_idx_start + 528\n",
    "\tlane_prob_idx_start = lane_idx_end\n",
    "\tlane_prob_idx_end = lane_prob_idx_start + 8\n",
    "\troad_edges_idx_start = lane_prob_idx_end\n",
    "\troad_edges_idx_end = road_edges_idx_start + 264\n",
    "\tleads_idx_start = road_edges_idx_end\n",
    "\tleads_idx_end = leads_idx_start + 105\n",
    "\tmeta_idx_start = leads_idx_end\n",
    "\tmeta_idx_end = meta_idx_start + 88\n",
    "\tpose_idx_start = meta_idx_end\n",
    "\tpose_idx_end = pose_idx_start + 12\n",
    "\twide_from_device_euler_idx_start = pose_idx_end\n",
    "\twide_from_device_euler_idx_end = wide_from_device_euler_idx_start + 6\n",
    "\ttemporal_pose_idx_start = wide_from_device_euler_idx_end\n",
    "\ttemporal_pose_idx_end = temporal_pose_idx_start + 12\n",
    "\troad_transform_idx_start = temporal_pose_idx_end\n",
    "\troad_transform_idx_end = road_transform_idx_start + 12\n",
    "\tfeature_buffer_idx_start = road_transform_idx_end\n",
    "\tfeature_buffer_idx_end = feature_buffer_idx_start + 128\n",
    "\tpadding_idx_start = feature_buffer_idx_end\n",
    "\tpadding_idx_end = padding_idx_start + 2\n",
    "\treturn {\"plans\": output[plan_idx_start:plan_idx_end],\n",
    "\t\t\t\"lanes\": output[lane_idx_start:lane_idx_end].flatten(),\n",
    "\t\t\t\"lanes_probs\": output[lane_prob_idx_start:lane_prob_idx_end],\n",
    "\t\t\t\"road_edges\": output[road_edges_idx_start:road_edges_idx_end],\n",
    "\t\t\t\"leads\": output[leads_idx_start:leads_idx_end],\n",
    "\t\t\t\"meta\": output[meta_idx_start:meta_idx_end],\n",
    "\t\t\t\"pose\": output[pose_idx_start:pose_idx_end],\n",
    "\t\t\t\"wide_euler\": output[wide_from_device_euler_idx_start:wide_from_device_euler_idx_end],\n",
    "\t\t\t\"temporal_pose\": output[temporal_pose_idx_start:temporal_pose_idx_end],\n",
    "\t\t\t\"road_transform\": output[road_transform_idx_start:road_transform_idx_end],\n",
    "\t\t\t\"feature_buffer\": output[feature_buffer_idx_start:feature_buffer_idx_end],\n",
    "\t\t\t\"padding\": output[padding_idx_start:padding_idx_end]}\n",
    "\n",
    "def place_patch(frames, patch, patch_size=(50, 50), eot_locations=[], eot_rotations=[], eot_scales=[]):\n",
    "\t\"\"\"\n",
    "\tPlaces a patch on 2 consecutive frames with Expectation over Transform (EoT).\n",
    "\n",
    "\tParameters:\n",
    "\t- frames: List of 2 tensors of shape (N, C, H, W), the batch of frames.\n",
    "\t- patch: Tensor of shape (N, C, H_patch, W_patch), the patch to place.\n",
    "\t- patch_size: Tuple (H_patch, W_patch), the size of the patch.\n",
    "\t- eot_locations: List of tuples [(x, y)], locations to place the patch.\n",
    "\t- eot_rotations: List of angles in degrees to rotate the patch.\n",
    "\t- eot_scales: List of scale factors to resize the patch.\n",
    "\n",
    "\tReturns:\n",
    "\t- frames_patches: List of 2 lists of transformed frames with patches applied for consecutive frames.\n",
    "\t\"\"\"\n",
    "\tframes_patches = []\n",
    "\tfor frame in frames:\n",
    "\t\tframe_transforms = []\n",
    "\t\tfor (x, y) in eot_locations:\n",
    "\t\t\tfor rotation in eot_rotations:\n",
    "\t\t\t\tfor scale in eot_scales:\n",
    "\t\t\t\t\t# Clone the frame\n",
    "\t\t\t\t\tframe_with_patch = frame.clone()\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t# Resize (scale) the patch\n",
    "\t\t\t\t\tscaled_patch = F.interpolate(patch, scale_factor=scale, mode='bilinear', align_corners=False)\n",
    "\n",
    "\t\t\t\t\t# Calculate new patch size after scaling\n",
    "\t\t\t\t\tnew_H_patch, new_W_patch = scaled_patch.shape[2], scaled_patch.shape[3]\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t# Create an affine transformation matrix for rotation\n",
    "\t\t\t\t\ttheta = torch.tensor([\n",
    "\t\t\t\t\t\t[torch.cos(torch.tensor(rotation)), -torch.sin(torch.tensor(rotation)), 0],\n",
    "\t\t\t\t\t\t[torch.sin(torch.tensor(rotation)), torch.cos(torch.tensor(rotation)), 0]\n",
    "\t\t\t\t\t], dtype=torch.float32)\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t# Grid for sampling\n",
    "\t\t\t\t\tgrid = F.affine_grid(theta.unsqueeze(0), scaled_patch.size(), align_corners=False)\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t# Apply the affine transformation (rotation)\n",
    "\t\t\t\t\trotated_patch = F.grid_sample(scaled_patch, grid, mode='bilinear', padding_mode='zeros', align_corners=False)\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t# Place the rotated and scaled patch onto the frame\n",
    "\t\t\t\t\tframe_with_patch[:, :, y:y + new_H_patch, x:x + new_W_patch] = rotated_patch\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t# Append the transformed frame to the list\n",
    "\t\t\t\t\tframe_transforms.append(frame_with_patch)\n",
    "\t\t\n",
    "\t\tframes_patches.append(frame_transforms)\n",
    "\n",
    "\treturn frames_patches\n",
    "\n",
    "def display_img(image):\n",
    "\tplt.imshow(image)\n",
    "\tplt.axis('off')  # Turn off axis\n",
    "\tplt.show()\n",
    "\tplt.clf()\n",
    "\n",
    "def numpy_to_tensor(array):\n",
    "\t# Convert image from BGR to RGB as PyTorch uses RGB by default\n",
    "\tframe_rgb = cv2.cvtColor(array, cv2.COLOR_BGR2RGB)\n",
    "\t# Convert to float32 for precision, then to float16\n",
    "\ttensor = torch.tensor(frame_rgb, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0)\n",
    "\treturn tensor.to(torch.float16)\n",
    "\n",
    "def tensor_to_numpy(tensor):\n",
    "\t# Convert back to float32 to avoid overflow when converting to uint8\n",
    "\ttensor_float32 = tensor.squeeze(0).permute(1, 2, 0).to(torch.float32)\n",
    "\timage_back = tensor_float32.detach().numpy().astype(np.uint8)\n",
    "\t# Convert RGB back to BGR\n",
    "\treturn cv2.cvtColor(image_back, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "def check_images(frame):\n",
    "\t# Step 1: Load the image using OpenCV\n",
    "\t#frame = cv2.imread(data_dir + frame_name)\n",
    "\n",
    "\t# Step 2: Convert the image to a PyTorch tensor in float16\n",
    "\t# Convert image from BGR to RGB as PyTorch uses RGB by default\n",
    "\tframe_rgb = numpy_to_tensor(frame)\n",
    "\n",
    "\t# Step 3: Convert the tensor back to a NumPy array\n",
    "\t# Convert back to float32 to avoid overflow when converting to uint8\n",
    "\timage_back_bgr = tensor_to_numpy(frame_rgb)\n",
    "\n",
    "\t# Check if both images are the same\n",
    "\tassert np.array_equal(frame, image_back_bgr), \"The images are not the same!\"\n",
    "\tdisplay_img(frame)\n",
    "\tdisplay_img(image_back_bgr)\n",
    "\n",
    "def disappearance_loss(patch, conf, patchDist, realDist, l1=0.01, l2=0.001):\n",
    "\tLconf = -torch.log(1 - conf) # 1-conf ya que se busca minimizar conf\n",
    "\tLdist = -torch.abs(patchDist/realDist)\n",
    "\t# Compute differences along height and width\n",
    "\tdiff_h = patch[:, :, 1:, :] - patch[:, :, :-1, :]\n",
    "\tdiff_w = patch[:, :, :, 1:] - patch[:, :, :, :-1]\n",
    "\tLtv = torch.sum(torch.abs(diff_h)) + torch.sum(torch.abs(diff_w))\n",
    "\treturn Lconf + l1*Ldist #+ l2*Ltv\n",
    "\n",
    "width = 512\n",
    "height = 256\n",
    "dim = (height, width)\n",
    "data_dir = 'frames/'\n",
    "frames_names = os.listdir(data_dir)\n",
    "parsed_images = []\n",
    "epochs = 10\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Sort frame filenames, by default 100.png is before 20.png\n",
    "frames_names = sorted(frames_names, key=lambda x: int(re.search(r'\\d+', x).group()))\n",
    "\n",
    "# Initialize randomly an adversarial patch and scale from [0,1] to [0,255]\n",
    "patch_size = (50, 50)\n",
    "adversarial_patch = nn.Parameter(torch.rand(1, 3, patch_size[0], patch_size[1], requires_grad=True)*255) #255 demasiado grande para optimizacion\n",
    "display_img(adversarial_patch.detach().squeeze(0).permute(1,2,0).numpy().astype(np.uint8))\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam([adversarial_patch], lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "# EoT: (x, y, w, h) 1928x1208 ¿¿rotation, scale??\n",
    "center_x = 964\n",
    "center_y = 604\n",
    "eot_locations = [(center_x-patch_size[0],center_y-patch_size[1]),\n",
    "                 (center_x-patch_size[0],center_y+patch_size[1]),\n",
    "                 (center_x,center_y),\n",
    "                 (center_x+patch_size[0],center_y-patch_size[1]),\n",
    "                 (center_x+patch_size[0],center_y+patch_size[1]),\n",
    "                ]\n",
    "eot_scales = [1, 2, 3]\n",
    "eot_rotations = [0]\n",
    "num_eot_transforms = len(eot_locations)*len(eot_rotations)*len(eot_rotations)\n",
    "\n",
    "def preprocess_frame(frame_tensor, roi_area=None, resize_dim=(128,256)):\n",
    "    x, y, w, h = roi_area\n",
    "    # Extract ROI (Region Of Interest) area of an image\n",
    "    roi_tensor = frame_tensor[:, :, y:y+h, x:x+w]\n",
    "    # Resize the images to the required dimensions\n",
    "    roi_tensor_resized = F.interpolate(roi_tensor, size=resize_dim, mode='bilinear', align_corners=False)\n",
    "    # Convert to YUV\n",
    "    roi_tensor_resized_yuv = rgb_to_yuv(roi_tensor_resized)\n",
    "    # Parse YUV with 6 channels: YUV_4:2:0\n",
    "    parsed_frame = parse_image(roi_tensor_resized_yuv)\n",
    "    return parsed_frame\n",
    "\n",
    "def euclidean_distance(pos1, pos2):\n",
    "    #return math.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2)\n",
    "    # Linf = torch.norm(v, p=float('inf'))\n",
    "    return torch.norm(torch.tensor(pos1) - torch.tensor(pos2), p=2).item() # L2 Norm = Euclidean distance\n",
    "      \n",
    "def convert_to_meters(normalized_y_position, scaling_factor=10):\n",
    "    \"\"\"\n",
    "    Converts the normalized Y position to real-world distance in meters.\n",
    "    \n",
    "    Parameters:\n",
    "    normalized_y_position: The Y position from the Supercombo output (normalized distance).\n",
    "    scaling_factor: A conversion factor to map the normalized Y position to meters (empirical or from camera calibration).\n",
    "    \n",
    "    Returns:\n",
    "    distance_meters: The longitudinal distance in meters.\n",
    "    \"\"\"\n",
    "    # Apply a scaling factor to convert normalized distance to meters\n",
    "    distance_meters = torch.abs(normalized_y_position) * scaling_factor  # Use abs() to ensure positive distance\n",
    "    return distance_meters\n",
    "\n",
    "def select_best_y(lead_predictions, alpha=1.0):\n",
    "    \"\"\"\n",
    "    Select the best lead vehicle longitudinal distance based on the y_mean and y_std values.\n",
    "    \n",
    "    Parameters:\n",
    "    - lead_predictions: A list of tuples, where each tuple contains (y_mean, y_std) as PyTorch tensors.\n",
    "    - alpha: A weighting factor to penalize high uncertainty (large y_std). Default is 1.0.\n",
    "    \n",
    "    Returns:\n",
    "    - best_y_mean: The best estimated longitudinal distance (y_mean) for the lead vehicle (as a PyTorch tensor).\n",
    "    \"\"\"\n",
    "    best_score = torch.tensor(float('inf'))  # Initialize best score as infinity\n",
    "    best_y_mean = None\n",
    "\n",
    "    for y_mean, y_std, prob in lead_predictions:\n",
    "        # Ensure y_mean and y_std are tensors\n",
    "        y_mean = torch.tensor(y_mean)\n",
    "        y_std = torch.tensor(y_std)\n",
    "\n",
    "        # Calculate a score that penalizes predictions with large y_std\n",
    "        score = y_mean + alpha * y_std\n",
    "\n",
    "        # Select the prediction with the lowest score\n",
    "        if score < best_score:\n",
    "            best_score = score\n",
    "            best_y_mean = y_mean\n",
    "\n",
    "    return best_y_mean\n",
    "\n",
    "def extract_lead_predictions(output, lead_idx_start=5755, lead_idx_end=5860):\n",
    "    # https://github.com/commaai/openpilot/blob/fa310d9e2542cf497d92f007baec8fd751ffa99c/selfdrive/modeld/models/driving.h\n",
    "    # Constants\n",
    "    DESIRE_PRED_SIZE = 32\n",
    "    OTHER_META_SIZE = 4\n",
    "\n",
    "    MODEL_WIDTH = 512\n",
    "    MODEL_HEIGHT = 256\n",
    "    MODEL_FRAME_SIZE = MODEL_WIDTH * MODEL_HEIGHT * 3 // 2\n",
    "\n",
    "    PLAN_MHP_N = 5\n",
    "    PLAN_MHP_COLUMNS = 30\n",
    "    PLAN_MHP_VALS = 30 * 33\n",
    "    PLAN_MHP_SELECTION = 1\n",
    "    PLAN_MHP_GROUP_SIZE = 2 * PLAN_MHP_VALS + PLAN_MHP_SELECTION\n",
    "\n",
    "    LEAD_MHP_N = 5\n",
    "    LEAD_MHP_VALS = 4\n",
    "    LEAD_MHP_SELECTION = 3\n",
    "    LEAD_MHP_GROUP_SIZE = 2 * LEAD_MHP_VALS + LEAD_MHP_SELECTION\n",
    "\n",
    "    POSE_SIZE = 12\n",
    "\n",
    "    # Index calculations\n",
    "    PLAN_IDX = 0\n",
    "    LL_IDX = PLAN_IDX + PLAN_MHP_N * PLAN_MHP_GROUP_SIZE\n",
    "    LL_PROB_IDX = LL_IDX + 4 * 2 * 2 * 33\n",
    "    RE_IDX = LL_PROB_IDX + 4\n",
    "    LEAD_IDX = RE_IDX + 2 * 2 * 2 * 33\n",
    "    LEAD_PROB_IDX = LEAD_IDX + LEAD_MHP_N * LEAD_MHP_GROUP_SIZE\n",
    "    DESIRE_STATE_IDX = LEAD_PROB_IDX + 3\n",
    "    DESIRE_LEN = 8  # Replace with the actual value of DESIRE_LEN\n",
    "    META_IDX = DESIRE_STATE_IDX + DESIRE_LEN\n",
    "    POSE_IDX = META_IDX + OTHER_META_SIZE + DESIRE_PRED_SIZE\n",
    "    OUTPUT_SIZE = POSE_IDX + POSE_SIZE\n",
    "\n",
    "    lead_data = output[LEAD_IDX:LEAD_PROB_IDX] # 10701 --> 10755\n",
    "    lead_probs = output[LEAD_PROB_IDX:DESIRE_STATE_IDX] # 10756 --> 10758\n",
    "    #print(\"-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.\")\n",
    "    #print(lead_data)\n",
    "    #print(lead_probs)\n",
    "    #print(\"-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.\")\n",
    "    # LEAD DATA:\n",
    "    # x: lateral\n",
    "    # y: longitudinal distance\n",
    "    # velocity\n",
    "    # acceleration\n",
    "    # (y_mean, y_std)\n",
    "    lead_pos_indexes = torch.tensor([(i*LEAD_MHP_GROUP_SIZE+1,i*LEAD_MHP_GROUP_SIZE+5, i*LEAD_MHP_GROUP_SIZE+8, i*LEAD_MHP_GROUP_SIZE+9, i*LEAD_MHP_GROUP_SIZE+10) for i in range(0,LEAD_MHP_N)])\n",
    "    lead_distances = [(lead_data[y_m_idx], lead_data[y_std_idx], torch.mean(torch.cat((lead_data[p1].unsqueeze(0), lead_data[p2].unsqueeze(0), lead_data[p3].unsqueeze(0)), dim=0))) for (y_m_idx,y_std_idx, p1, p2, p3) in lead_pos_indexes]\n",
    "    \"\"\"for lead_pos in lead_distances:\n",
    "        print(lead_pos)\n",
    "    print(\"--------------------------------------------------------------------\")\"\"\"\n",
    "    #y_best = select_best_y(lead_distances)\n",
    "    y_best = lead_data[1]\n",
    "    distance = convert_to_meters(y_best, scaling_factor=10.0)\n",
    "    best_prob = torch.max(torch.sigmoid(lead_probs))\n",
    "    return distance, best_prob\n",
    "\n",
    "def compare_onnx_torch_outputs(onnx_output, torch_output):\n",
    "    onnx_output = np.array(onnx_output)\n",
    "    torch_output = torch_output.detach().cpu().numpy()\n",
    "    assert torch_output.shape == onnx_output.shape, \"[!] Error: ONNX and Torch output shapes should be the same! (1, 6210)\"\n",
    "    diff = torch_output[0].detach().numpy() - onnx_output[0]\n",
    "    print(f\"Num equals: {np.sum(np.abs(diff == 0))}\")\n",
    "    print(f\"Num differents: {np.sum(np.abs(diff != 0))}\")\n",
    "    print(f\"Mean Overall (6120): {np.mean(diff)}\")\n",
    "    print(f\"Std Overall (6120): {np.std(diff)}\")\n",
    "    print(f\"Mean differents ({np.sum(diff != 0)}): {np.mean(np.array(diff != 0))}\")\n",
    "    print(f\"Std differents ({np.sum(diff != 0)}): {np.std(np.array(diff != 0))}\")\n",
    "    # TODO: mean y std\n",
    "    # Visualize differences\n",
    "    small_range = diff[np.abs(diff) <= 1e-5]\n",
    "    print(small_range.shape)\n",
    "    #small_range = diff[np.abs(diff) <= 1e-8]\n",
    "    print(small_range.shape)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(small_range, bins=100, edgecolor='black')\n",
    "    plt.title('Histogram of Differences between ONNX and PyTorch Outputs')\n",
    "    plt.xlabel('Difference')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "def subplot(img1, img2):\n",
    "\t# Create a figure with subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "    # Display the frame in the first subplot\n",
    "    ax1.imshow(img1)\n",
    "    ax1.set_title('Original Frame')\n",
    "    ax1.axis('off')\n",
    "\n",
    "    # Display the adversarial patch in the second subplot\n",
    "    ax2.imshow(img2)\n",
    "    ax2.set_title('Adversarial Patch')\n",
    "    ax2.axis('off')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "original_input_imgs_data = None # Shape: (1, 12, 128, 256)\n",
    "patched_input_imgs_data = None # Shape: (1, 12, 128, 256)\n",
    "desire_data = np.array([0]).astype('float16')\n",
    "desire_data.resize((1,8))\n",
    "traffic_convention_data = np.array([1, 0]).astype('float16')\n",
    "traffic_convention_data.resize((1,2))\n",
    "initial_state_data = np.array([0]).astype('float16')\n",
    "initial_state_data.resize((1,512))\n",
    "\n",
    "# Initialize an empty DataFrame with the appropriate column names\n",
    "columns = ['Patch', 'Batch_rdist', 'Batch_rconf', 'Batch_pdist', 'Batch_pconf', 'Batch_loss']\n",
    "train_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    for f_idx, frame_name in enumerate(frames_names):\n",
    "        # Prepare frame: convert to YUV420 and resize dimensions\n",
    "        # Load frame in BGR format\n",
    "        frame = cv2.imread(data_dir + frame_name) # Los PNG se ven azules porque estan en RGB cuando en realidad se usa BGR\n",
    "        #display_img(frame)\n",
    "        # Convert numpy frame to torch tensor\n",
    "        frame_tensor = numpy_to_tensor(frame)  # Shape: [1, 3, H, W] of float16 in RGB\n",
    "        # Only keep the 2 consecutive new frames: delete the oldest\n",
    "        if (len(parsed_images) >= 2):\n",
    "            del parsed_images[0]\n",
    "        parsed_images.append(frame_tensor)\n",
    "        if (len(parsed_images) >= 2):\n",
    "            # Location and crop area: https://github.com/commaai/openpilot/blob/fa310d9e2542cf497d92f007baec8fd751ffa99c/system/camerad/cameras/camera_qcom2.cc#L1252\n",
    "            original_road_old = preprocess_frame(parsed_images[0], (96, 160, 1734, 986), dim)\n",
    "            original_road_new = preprocess_frame(parsed_images[1], (96, 160, 1734, 986), dim)\n",
    "            # Join the two parsed frames for original \"input_imgs\" input\n",
    "            original_input_imgs_data = torch.cat([original_road_old, original_road_new], dim=1) # Shape: (1, 12, 128, 256)\n",
    "            original_result = torch_model(torch.as_tensor(original_input_imgs_data, dtype=torch.float),\n",
    "                                        torch.as_tensor(desire_data, dtype=torch.float),\n",
    "                                        torch.as_tensor(traffic_convention_data, dtype=torch.float),\n",
    "                                        torch.as_tensor(initial_state_data, dtype=torch.float))\n",
    "            # -- Comparison outputs --\n",
    "            #onnx_input = parse_input(session, (original_input_imgs_data.detach().numpy().astype(\"float16\")))\n",
    "            \"\"\"onnx_result = np.array(session.run([session.get_outputs()[0].name], {\"input_imgs\": original_input_imgs_data.detach().numpy().astype(\"float32\"),\n",
    "                                                                                 \"desire\": desire_data.astype(\"float32\"),\n",
    "                                                                                 \"traffic_convention\": traffic_convention_data.astype(\"float32\"),\n",
    "                                                                                 \"initial_state\": initial_state_data.astype(\"float32\")}))[0]\"\"\"                                                                                \n",
    "            #compare_onnx_torch_outputs(onnx_result, original_result)\n",
    "            #print(original_result.shape)\n",
    "            #print(onnx_result.shape)\n",
    "            #sys.exit(0)\n",
    "            #--------------------------\n",
    "            #original_result = original_result.cpu().detach().numpy() # Shape (1, 6120)\n",
    "            #original_res = parse_output(original_result[0]) # Dict\n",
    "        # Apply EoT transformations: returns an array of the two consecutive frames with different patch transformations\n",
    "        batch = place_patch(parsed_images, adversarial_patch, patch_size, eot_locations, eot_rotations, eot_scales)\n",
    "        # EoT: Expectation Over Transform\n",
    "        batch_rdist = torch.tensor([])\n",
    "        batch_rconf = torch.tensor([])\n",
    "        batch_pdist = torch.tensor([])\n",
    "        batch_pconf = torch.tensor([])\n",
    "        batch_losses = torch.tensor([])\n",
    "        # Compare original frames with all EoT transformations\n",
    "        if (len(parsed_images) >= 2):\n",
    "            for transform_idx in range(0,num_eot_transforms):\n",
    "                #display_img(tensor_to_numpy(batch[0][transform_idx]))\n",
    "                parsed_patch_road_old = preprocess_frame(batch[0][transform_idx], (96, 160, 1734, 986), dim)\n",
    "                parsed_patch_road_new = preprocess_frame(batch[1][transform_idx], (96, 160, 1734, 986), dim)\n",
    "\n",
    "                # dim=1 is used to get shape (1,12,128,256) if dim=0 then (2,6,128,256)\n",
    "                # Join the two parsed frames for patched \"input_imgs\" input\n",
    "                patched_input_imgs_data = torch.cat([parsed_patch_road_old, parsed_patch_road_new], dim=1) # Shape: (1, 12, 128, 256)\n",
    "\n",
    "                # Torch model query\n",
    "                patched_result = torch_model(torch.as_tensor(patched_input_imgs_data, dtype=torch.float),\n",
    "                                            torch.as_tensor(desire_data, dtype=torch.float),\n",
    "                                            torch.as_tensor(traffic_convention_data, dtype=torch.float),\n",
    "                                            torch.as_tensor(initial_state_data, dtype=torch.float))\n",
    "                #patched_result = patched_result.cpu().detach().numpy() # Shape (1, 6120)\n",
    "                #patched_res = parse_output(patched_result[0]) # Dict\n",
    "                #target_class_probabilities = predictions[:, target_class_index]\n",
    "                #loss = -torch.mean(torch.log(target_class_probabilities))\n",
    "                rDist, rConf = extract_lead_predictions(original_result[0])\n",
    "                pDist, pConf = extract_lead_predictions(patched_result[0])\n",
    "                #display_img(frame)\n",
    "                \"\"\"print(f\"Real Dist: {rDist.detach().cpu().numpy()}\")\n",
    "                print(f\"Real Conf: {rConf.detach().cpu().numpy()}\")\n",
    "                print(f\"Adv Dist: {pDist.detach().cpu().numpy()}\")\n",
    "                print(f\"Adv Conf: {pConf.detach().cpu().numpy()}\")\n",
    "                sys.exit(0)\"\"\"\n",
    "                tmploss = disappearance_loss(adversarial_patch, pConf, pDist, rDist).unsqueeze(0)\n",
    "                #print(tmploss)\n",
    "                batch_rdist = torch.cat((batch_rdist,rDist.unsqueeze(0)))\n",
    "                batch_rconf = torch.cat((batch_rconf,rConf.unsqueeze(0)))\n",
    "                batch_pdist = torch.cat((batch_pdist,pDist.unsqueeze(0)))\n",
    "                batch_pconf = torch.cat((batch_pconf,pConf.unsqueeze(0)))\n",
    "                batch_losses = torch.cat((batch_losses,tmploss))\n",
    "            # Calculate the expectation of the transforms and update patch\n",
    "            # Reset to zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            #print(adversarial_patch.grad)\n",
    "            # Calculate loss\n",
    "            loss = torch.mean(batch_losses)\n",
    "            #training_info.append((adversarial_patch.clone(), torch.mean(batch_rdist), torch.mean(batch_rconf), torch.mean(batch_pdist), torch.mean(batch_pconf), torch.mean(batch_losses)))\n",
    "            # Append data to the DataFrame\n",
    "            train_df = pd.concat([train_df, pd.DataFrame([{\n",
    "                'Patch': adversarial_patch.clone(),\n",
    "                'Batch_rdist': torch.mean(batch_rdist).item(),\n",
    "                'Batch_rconf': torch.mean(batch_rconf).item(),\n",
    "                'Batch_pdist': torch.mean(batch_pdist).item(),\n",
    "                'Batch_pconf': torch.mean(batch_pconf).item(),\n",
    "                'Batch_loss': torch.mean(batch_losses).item()\n",
    "            }])], ignore_index=True)\n",
    "            \"\"\"subplot(frame, adversarial_patch.detach().squeeze(0).permute(1,2,0).numpy().astype(np.uint8))\n",
    "            print(f\"Epoch: {epoch} ; Frame: {f_idx}\")\n",
    "            print(\"------------------------------------\")\n",
    "            print(f\"Real Dist: {torch.mean(batch_rdist)}\")\n",
    "            print(f\"Real Conf: {torch.mean(batch_rconf)}\")\n",
    "            print(f\"Adv Dist: {torch.mean(batch_pdist)}\")\n",
    "            print(f\"Adv Conf: {torch.mean(batch_pconf)}\")\n",
    "            print(f\"Loss: {loss}\")\n",
    "            print()\"\"\"\n",
    "            print(f\"{epoch}:{f_idx} ; {loss.item()} ; {torch.mean(batch_pdist).item()} ; {torch.mean(batch_pconf).item()}\")\n",
    "            # Update learning rate\n",
    "            if torch.mean(batch_pconf) < torch.tensor(0.6):\n",
    "                scheduler.step()\n",
    "            #print(adversarial_patch.grad)\n",
    "            loss.backward() # Compute gradients of the mean loss\n",
    "            #print(adversarial_patch.grad)\n",
    "            #print(\"------------------------------------------------------\")\n",
    "            # Update patch\n",
    "            optimizer.step() # Update target tensor (adversarial_example) parameters based on computed gradients\n",
    "            #dpatch = adversarial_patch.clone()\n",
    "            #display_img(tensor_to_numpy(dpatch))\n",
    "    print(f\"[*] Epoch {epoch}...\")\n",
    "print(f\"[*] DONE: Adversarial Patch Trained\")\n",
    "display_img(adversarial_patch.detach().squeeze(0).permute(1,2,0).numpy().astype(np.uint8))\n",
    "display_img(tensor_to_numpy(adversarial_patch))\n",
    "# Retrieve outputs: https://github.com/MTammvee/openpilot-supercombo-model/blob/main/openpilot_onnx.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_img(tensor_to_numpy(adversarial_patch.clone()))\n",
    "print(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
